xtrain <- x[trainIndex,1:ncol(x)]
set.seed(1234)
cv_fit <- cv.glmnet(xtrain, ytrain,alpha=1, nfolds = 5)
plot(cv_fit)
plot(cv_fit$glmnet.fit, xvar = "lambda")
coef_min <- coef(cv_fit, s = "lambda.min")
sel_vars_min <- rownames(coef_min)[which(coef_min != 0)][-1]
sel_vars_min
data_df=data_rel |> select(c(Fire_Duration, all_of(cov_columns_ind)))
data_df <- data_df |> na.omit()
y <- data_df$Fire_Duration
x <- model.matrix(Fire_Duration ~ . - 1, data=data_df)
set.seed(1234)
trainIndex <- sample(1:nrow(x), nrow(x)*0.9)
# Train Model Matrix
ytrain <- y[trainIndex]
xtrain <- x[trainIndex,sel_vars_min]
train_df <- as.data.frame(xtrain) |> mutate(Fire_Duration = ytrain)
# Test Model Matrix
ytest <- y[-trainIndex]
xtest <- x[-trainIndex,sel_vars_min]
test_df <- as.data.frame(xtest) |> mutate(Fire_Duration = ytest)
# Cleaning the data
train_df <- clean_names(train_df)
test_df <- clean_names(test_df)
ffit_lm <- lm(fire_duration ~ ., data = train_df)
summary(ffit_lm)
pred.test <- predict(ffit_lm, newdata = test_df)
mean((pred.test-ytest)**2)
test_TSS <- sum((test_df$fire_duration - mean(test_df$fire_duration))**2)
test_RSS <- sum((pred.test - test_df$fire_duration)**2)
cat(round(1 - test_RSS/test_TSS,3))
ggplot() +
geom_point(mapping=aes(x=ffit_lm$fitted.values, y=ffit_lm$residuals), color="skyblue") +
xlab("Fitted Values")+
ylab("Residuals")+
theme_bw()
colnames(train_df)
formula_gam <- as.formula(
paste("fire_duration ~ s(`max_temp`) + s(`min_temp`) + s(`avg_temp`) + s(`avg_windspeed`)  + s(`elevation`) +
s(`avg_spi30d`) + `fuel_type_shrubland` + `fuel_type_urban` +
`cause_description_fireworks` + `cause_description_natural` + `cause_description_unknown` +
`cause_description_unknown_human` "))
ffit_gam <- gam(formula_gam, data = train_df)
plot(ffit_gam)
pred.test <- predict(ffit_gam, newdata = test_df)
mean((pred.test-ytest)**2)
test_TSS <- sum((test_df$fire_duration - mean(test_df$fire_duration))**2)
test_RSS <- sum((pred.test - test_df$fire_duration)**2)
cat(round(1 - test_RSS/test_TSS,3))
set.seed(123)
rf_model <- randomForest(
fire_duration ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df))),
importance = TRUE
)
summary(rf_model)
pred.test <- predict(rf_model, newdata = test_df)
mean((pred.test-ytest)**2)
test_TSS <- sum((test_df$fire_duration - mean(test_df$fire_duration))**2)
test_RSS <- sum((pred.test - test_df$fire_duration)**2)
cat(round(1 - test_RSS/test_TSS,3))
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(fire_duration ~ ., data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
varImpPlot(rf_model)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(car)
library(janitor)
data <- read.csv("../Data/WildFire_DataSet.csv")
data_rel <- data |> filter(Fire_Duration>15)
dat <- data |>
mutate(Acresbuckets = case_when(
incident_acres_burned <= 100 ~ 0,
incident_acres_burned > 100 & incident_acres_burned <= 500 ~ 1,
incident_acres_burned > 500 ~ 2
))
dat$Acresbuckets <- as.factor(dat$Acresbuckets)
dat |> group_by(Acresbuckets) |>
summarize(n=n())
dat |>
ggplot(mapping=aes(x=Fire_Duration, y=log_acres_burned))+
geom_point()
cov_columns_new <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "landcover", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df_0 = dat |> select(c(log_acres_burned, all_of(cov_columns_new)))
data_df_0 <- data_df_0 |> na.omit()
set.seed(1234)
trainIndex <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.9)
# Train Model Matrix
train_df <- data_df_0[trainIndex,1:ncol(data_df_0)]
# Test Model Matrix
test_df <- data_df_0[-trainIndex,1:ncol(data_df_0)]
fit_lm <- lm(log_acres_burned ~ . -cause_description, data=train_df)
summary(fit_lm)
vif(fit_lm)
set.seed(123)
rf_model_fd <- randomForest(
log_acres_burned ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df) - 1)),
importance = TRUE
)
summary(rf_model_fd)
importance(rf_model_fd)
# Getting R2 value
pred.test <- predict(rf_model_fd, newdata = test_df)
test_TSS <- sum((test_df$log_acres_burned - mean(test_df$log_acres_burned))**2)
test_RSS <- sum((pred.test - test_df$log_acres_burned)**2)
cat(round(1 - test_RSS/test_TSS,3))
cov_columns_class <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df = dat |> select(c(Acresbuckets, all_of(cov_columns_class)))
data_df <- data_df |> na.omit()
set.seed(1234)
trainIndex_cls <- sample(1:nrow(data_df), nrow(data_df)*0.9)
# Train Model Matrix
train_df <- data_df[trainIndex_cls,1:ncol(data_df)]
# Test Model Matrix
test_df <- data_df[-trainIndex_cls,1:ncol(data_df)]
data_df |> group_by(Acresbuckets) |>
summarize(n=n())
library(nnet)
fit_nnet <- multinom(Acresbuckets ~ ., data=train_df)
summary(fit_nnet)
# Predict on test data
test_preds <- predict(fit_nnet, newdata = test_df)
mean(predict(fit_nnet, newdata = test_df) != test_df$Acresbuckets)
mean(predict(fit_nnet, newdata = train_df) != train_df$Acresbuckets)
table(test_preds, test_df$Acresbuckets)
set.seed(123)
train_error <- rep(NA,ncol(train_df)-1)
test_error <- rep(NA, ncol(test_df)-1)
p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(car)
library(janitor)
data <- read.csv("../Data/WildFire_DataSet.csv")
data_rel <- data |> filter(Fire_Duration>15)
dat <- data |>
mutate(Acresbuckets = case_when(
incident_acres_burned <= 100 ~ 0,
incident_acres_burned > 100 & incident_acres_burned <= 500 ~ 1,
incident_acres_burned > 500 ~ 2
))
dat$Acresbuckets <- as.factor(dat$Acresbuckets)
dat |> group_by(Acresbuckets) |>
summarize(n=n())
dat |>
ggplot(mapping=aes(x=Fire_Duration, y=log_acres_burned))+
geom_point()
cov_columns_new <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "landcover", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df_0 = dat |> select(c(log_acres_burned, all_of(cov_columns_new)))
data_df_0 <- data_df_0 |> na.omit()
set.seed(1234)
trainIndex <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.9)
# Train Model Matrix
train_df <- data_df_0[trainIndex,1:ncol(data_df_0)]
# Test Model Matrix
test_df <- data_df_0[-trainIndex,1:ncol(data_df_0)]
fit_lm <- lm(log_acres_burned ~ . -cause_description, data=train_df)
summary(fit_lm)
vif(fit_lm)
set.seed(123)
rf_model_fd <- randomForest(
log_acres_burned ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df) - 1)),
importance = TRUE
)
summary(rf_model_fd)
importance(rf_model_fd)
# Getting R2 value
pred.test <- predict(rf_model_fd, newdata = test_df)
test_TSS <- sum((test_df$log_acres_burned - mean(test_df$log_acres_burned))**2)
test_RSS <- sum((pred.test - test_df$log_acres_burned)**2)
cat(round(1 - test_RSS/test_TSS,3))
cov_columns_class <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df = dat |> select(c(Acresbuckets, all_of(cov_columns_class)))
data_df <- data_df |> na.omit()
set.seed(1234)
trainIndex_cls <- sample(1:nrow(data_df), nrow(data_df)*0.9)
# Train Model Matrix
train_df <- data_df[trainIndex_cls,1:ncol(data_df)]
# Test Model Matrix
test_df <- data_df[-trainIndex_cls,1:ncol(data_df)]
data_df |> group_by(Acresbuckets) |>
summarize(n=n())
library(nnet)
fit_nnet <- multinom(Acresbuckets ~ ., data=train_df)
summary(fit_nnet)
# Predict on test data
test_preds <- predict(fit_nnet, newdata = test_df)
mean(predict(fit_nnet, newdata = test_df) != test_df$Acresbuckets)
mean(predict(fit_nnet, newdata = train_df) != train_df$Acresbuckets)
table(test_preds, test_df$Acresbuckets)
set.seed(123)
train_error <- rep(NA,ncol(train_df)-1)
test_error <- rep(NA, ncol(test_df)-1)
p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(car)
library(janitor)
data <- read.csv("../Data/WildFire_DataSet.csv")
data_rel <- data |> filter(Fire_Duration>15)
dat <- data |>
mutate(Acresbuckets = case_when(
incident_acres_burned <= 100 ~ 0,
incident_acres_burned > 100 & incident_acres_burned <= 500 ~ 1,
incident_acres_burned > 500 ~ 2
))
dat$Acresbuckets <- as.factor(dat$Acresbuckets)
dat |> group_by(Acresbuckets) |>
summarize(n=n())
dat |>
ggplot(mapping=aes(x=Fire_Duration, y=log_acres_burned))+
geom_point()
cov_columns_new <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "landcover", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df_0 = dat |> select(c(log_acres_burned, all_of(cov_columns_new)))
data_df_0 <- data_df_0 |> na.omit()
set.seed(1234)
trainIndex <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.9)
# Train Model Matrix
train_df <- data_df_0[trainIndex,1:ncol(data_df_0)]
# Test Model Matrix
test_df <- data_df_0[-trainIndex,1:ncol(data_df_0)]
fit_lm <- lm(log_acres_burned ~ . -cause_description, data=train_df)
summary(fit_lm)
vif(fit_lm)
set.seed(123)
rf_model_fd <- randomForest(
log_acres_burned ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df) - 1)),
importance = TRUE
)
summary(rf_model_fd)
importance(rf_model_fd)
# Getting R2 value
pred.test <- predict(rf_model_fd, newdata = test_df)
test_TSS <- sum((test_df$log_acres_burned - mean(test_df$log_acres_burned))**2)
test_RSS <- sum((pred.test - test_df$log_acres_burned)**2)
cat(round(1 - test_RSS/test_TSS,3))
cov_columns_class <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df = dat |> select(c(Acresbuckets, all_of(cov_columns_class)))
data_df <- data_df |> na.omit()
set.seed(1234)
trainIndex_cls <- sample(1:nrow(data_df), nrow(data_df)*0.9)
# Train Model Matrix
train_df <- data_df[trainIndex_cls,1:ncol(data_df)]
# Test Model Matrix
test_df <- data_df[-trainIndex_cls,1:ncol(data_df)]
data_df |> group_by(Acresbuckets) |>
summarize(n=n())
library(nnet)
fit_nnet <- multinom(Acresbuckets ~ ., data=train_df)
summary(fit_nnet)
# Predict on test data
test_preds <- predict(fit_nnet, newdata = test_df)
mean(predict(fit_nnet, newdata = test_df) != test_df$Acresbuckets)
mean(predict(fit_nnet, newdata = train_df) != train_df$Acresbuckets)
table(test_preds, test_df$Acresbuckets)
set.seed(123)
train_error <- rep(NA,ncol(train_df)-1)
test_error <- rep(NA, ncol(test_df)-1)
p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
