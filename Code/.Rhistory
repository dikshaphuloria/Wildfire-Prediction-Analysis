p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(car)
library(janitor)
data <- read.csv("../Data/WildFire_DataSet.csv")
data_rel <- data |> filter(Fire_Duration>15)
dat <- data |>
mutate(Acresbuckets = case_when(
incident_acres_burned <= 100 ~ 0,
incident_acres_burned > 100 & incident_acres_burned <= 500 ~ 1,
incident_acres_burned > 500 ~ 2
))
dat$Acresbuckets <- as.factor(dat$Acresbuckets)
dat |> group_by(Acresbuckets) |>
summarize(n=n())
dat |>
ggplot(mapping=aes(x=Fire_Duration, y=log_acres_burned))+
geom_point()
cov_columns_new <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "landcover", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df_0 = dat |> select(c(log_acres_burned, all_of(cov_columns_new)))
data_df_0 <- data_df_0 |> na.omit()
set.seed(1234)
trainIndex <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.9)
# Train Model Matrix
train_df <- data_df_0[trainIndex,1:ncol(data_df_0)]
# Test Model Matrix
test_df <- data_df_0[-trainIndex,1:ncol(data_df_0)]
fit_lm <- lm(log_acres_burned ~ . -cause_description, data=train_df)
summary(fit_lm)
vif(fit_lm)
set.seed(123)
rf_model_fd <- randomForest(
log_acres_burned ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df) - 1)),
importance = TRUE
)
summary(rf_model_fd)
importance(rf_model_fd)
# Getting R2 value
pred.test <- predict(rf_model_fd, newdata = test_df)
test_TSS <- sum((test_df$log_acres_burned - mean(test_df$log_acres_burned))**2)
test_RSS <- sum((pred.test - test_df$log_acres_burned)**2)
cat(round(1 - test_RSS/test_TSS,3))
cov_columns_class <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df = dat |> select(c(Acresbuckets, all_of(cov_columns_class)))
data_df <- data_df |> na.omit()
set.seed(1234)
trainIndex_cls <- sample(1:nrow(data_df), nrow(data_df)*0.9)
# Train Model Matrix
train_df <- data_df[trainIndex_cls,1:ncol(data_df)]
# Test Model Matrix
test_df <- data_df[-trainIndex_cls,1:ncol(data_df)]
data_df |> group_by(Acresbuckets) |>
summarize(n=n())
library(nnet)
fit_nnet <- multinom(Acresbuckets ~ ., data=train_df)
summary(fit_nnet)
# Predict on test data
test_preds <- predict(fit_nnet, newdata = test_df)
mean(predict(fit_nnet, newdata = test_df) != test_df$Acresbuckets)
mean(predict(fit_nnet, newdata = train_df) != train_df$Acresbuckets)
table(test_preds, test_df$Acresbuckets)
set.seed(123)
train_error <- rep(NA,ncol(train_df)-1)
test_error <- rep(NA, ncol(test_df)-1)
p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(car)
library(janitor)
data <- read.csv("../Data/WildFire_DataSet.csv")
data_rel <- data |> filter(Fire_Duration>15)
dat <- data |>
mutate(Acresbuckets = case_when(
incident_acres_burned <= 100 ~ 0,
incident_acres_burned > 100 & incident_acres_burned <= 500 ~ 1,
incident_acres_burned > 500 ~ 2
))
dat$Acresbuckets <- as.factor(dat$Acresbuckets)
dat |> group_by(Acresbuckets) |>
summarize(n=n())
dat |>
ggplot(mapping=aes(x=Fire_Duration, y=log_acres_burned))+
geom_point()
cov_columns_new <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "landcover", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df_0 = dat |> select(c(log_acres_burned, all_of(cov_columns_new)))
data_df_0 <- data_df_0 |> na.omit()
set.seed(1234)
trainIndex <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.9)
# Train Model Matrix
train_df <- data_df_0[trainIndex,1:ncol(data_df_0)]
# Test Model Matrix
test_df <- data_df_0[-trainIndex,1:ncol(data_df_0)]
fit_lm <- lm(log_acres_burned ~ . -cause_description, data=train_df)
summary(fit_lm)
vif(fit_lm)
set.seed(123)
rf_model_fd <- randomForest(
log_acres_burned ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df) - 1)),
importance = TRUE
)
summary(rf_model_fd)
importance(rf_model_fd)
# Getting R2 value
pred.test <- predict(rf_model_fd, newdata = test_df)
test_TSS <- sum((test_df$log_acres_burned - mean(test_df$log_acres_burned))**2)
test_RSS <- sum((pred.test - test_df$log_acres_burned)**2)
cat(round(1 - test_RSS/test_TSS,3))
cov_columns_class <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df = dat |> select(c(Acresbuckets, all_of(cov_columns_class)))
data_df <- data_df |> na.omit()
set.seed(1234)
trainIndex_cls <- sample(1:nrow(data_df), nrow(data_df)*0.9)
# Train Model Matrix
train_df <- data_df[trainIndex_cls,1:ncol(data_df)]
# Test Model Matrix
test_df <- data_df[-trainIndex_cls,1:ncol(data_df)]
data_df |> group_by(Acresbuckets) |>
summarize(n=n())
library(nnet)
fit_nnet <- multinom(Acresbuckets ~ ., data=train_df)
summary(fit_nnet)
# Predict on test data
test_preds <- predict(fit_nnet, newdata = test_df)
mean(predict(fit_nnet, newdata = test_df) != test_df$Acresbuckets)
mean(predict(fit_nnet, newdata = train_df) != train_df$Acresbuckets)
table(test_preds, test_df$Acresbuckets)
set.seed(123)
train_error <- rep(NA,ncol(train_df)-1)
test_error <- rep(NA, ncol(test_df)-1)
p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(car)
library(janitor)
data <- read.csv("../Data/WildFire_DataSet.csv")
data_rel <- data |> filter(Fire_Duration>15)
dat <- data |>
mutate(Acresbuckets = case_when(
incident_acres_burned <= 100 ~ 0,
incident_acres_burned > 100 & incident_acres_burned <= 500 ~ 1,
incident_acres_burned > 500 ~ 2
))
dat$Acresbuckets <- as.factor(dat$Acresbuckets)
dat |> group_by(Acresbuckets) |>
summarize(n=n())
dat |>
ggplot(mapping=aes(x=Fire_Duration, y=log_acres_burned))+
geom_point()
cov_columns_new <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "landcover", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df_0 = dat |> select(c(log_acres_burned, all_of(cov_columns_new)))
data_df_0 <- data_df_0 |> na.omit()
set.seed(1234)
trainIndex <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.9)
# Train Model Matrix
train_df <- data_df_0[trainIndex,1:ncol(data_df_0)]
# Test Model Matrix
test_df <- data_df_0[-trainIndex,1:ncol(data_df_0)]
fit_lm <- lm(log_acres_burned ~ . -cause_description, data=train_df)
summary(fit_lm)
vif(fit_lm)
set.seed(123)
rf_model_fd <- randomForest(
log_acres_burned ~ .,
data = train_df,
ntree = 1000,
mtry = floor(sqrt(ncol(train_df) - 1)),
importance = TRUE
)
summary(rf_model_fd)
importance(rf_model_fd)
# Getting R2 value
pred.test <- predict(rf_model_fd, newdata = test_df)
test_TSS <- sum((test_df$log_acres_burned - mean(test_df$log_acres_burned))**2)
test_RSS <- sum((pred.test - test_df$log_acres_burned)**2)
cat(round(1 - test_RSS/test_TSS,3))
cov_columns_class <- c("min_temp","max_temp","avg_temp","avg_windspeed","avg_precipitation", "elevation", "aspect", "slope", "ndvi", "avg_pdsi", "avg_spi30d", "ndmi","fuel_type", "cause_description")
data_df = dat |> select(c(Acresbuckets, all_of(cov_columns_class)))
data_df <- data_df |> na.omit()
set.seed(1234)
trainIndex_cls <- sample(1:nrow(data_df), nrow(data_df)*0.9)
# Train Model Matrix
train_df <- data_df[trainIndex_cls,1:ncol(data_df)]
# Test Model Matrix
test_df <- data_df[-trainIndex_cls,1:ncol(data_df)]
data_df |> group_by(Acresbuckets) |>
summarize(n=n())
library(nnet)
fit_nnet <- multinom(Acresbuckets ~ ., data=train_df)
summary(fit_nnet)
# Predict on test data
test_preds <- predict(fit_nnet, newdata = test_df)
mean(predict(fit_nnet, newdata = test_df) != test_df$Acresbuckets)
mean(predict(fit_nnet, newdata = train_df) != train_df$Acresbuckets)
table(test_preds, test_df$Acresbuckets)
set.seed(123)
train_error <- rep(NA,ncol(train_df)-1)
test_error <- rep(NA, ncol(test_df)-1)
p <- ncol(train_df)-1
for (m in 1:p){
rf.fit <- randomForest(Acresbuckets ~ .,data = train_df,
ntree = 500, mtry = m,importance = TRUE,maxnodes =10)
# Train and Test Error
train_error[m] <- mean(predict(rf.fit, newdata = train_df) != train_df$Acresbuckets)
test_error[m] <- mean(predict(rf.fit, newdata = test_df) != test_df$Acresbuckets)
}
par(bg = "lightgray")
plot(1:p, train_error, type = "b", col = "#12b59a", pch = 19,
xlab = "mtry",
ylab = "Misclassification Error",
main = "Train and Test Error vs mtry",
ylim = range(c(train_error, test_error)))
lines(1:p, test_error, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
set.seed(123)
library(randomForest)
# Fit the random forest model
rf_model_cls <- randomForest(
Acresbuckets ~ .,
data = train_df,
ntree = 1000,
mtry = 12,
importance = TRUE,
maxnodes = 10
)
mean(predict(rf_model_cls, newdata = train_df) != train_df$Acresbuckets)
mean(predict(rf_model_cls, newdata = test_df) != test_df$Acresbuckets)
y <- as.integer(data_df$Acresbuckets) - 1
X <- model.matrix(Acresbuckets ~ . -1, data=data_df)
set.seed(123)
trainIndex_xgb <- sample(1:nrow(data_df_0), nrow(data_df_0)*0.8)
# Train Matrix XGB
y_train_xgb <- y[trainIndex_xgb]
X_train <- X[trainIndex_xgb,1:ncol(X)]
# Test Matrix
y_test_xgb <- y[-trainIndex_xgb]
X_test <- X[-trainIndex_xgb,1:ncol(X)]
model <- xgboost(X_train, y_train_xgb, verbose=FALSE,
num_class = 3, nrounds = 10, params = list(objective="multi:softmax"))
# Test Error Rate
pred_class_index <- predict(model, newdata = X_test)
mean(pred_class_index != y_test_xgb)
n_rounds <- 10
train_error_xgb <- rep(NA, n_rounds)
test_error_xgb <- rep(NA, n_rounds)
set.seed(123)
for(num_round in 1:n_rounds){
cat(num_round,"\n")
model_xgb_cls <- xgboost(X_train, y_train_xgb, verbose=FALSE, num_class = 3, nrounds = num_round, params = list(max_depth=4, colsample_bytree=0.8, eta=0.5, objective="multi:softmax"))
# Train Data
pred_train_xgb <- predict(model_xgb_cls, newdata = X_train)
train_error_xgb[num_round] <- mean(pred_train_xgb!=y_train_xgb)
# Test Data
pred_test_xgb <- predict(model_xgb_cls, newdata = X_test)
test_error_xgb[num_round] <- mean(pred_test_xgb!=y_test_xgb)
}
plot(1:n_rounds, train_error_xgb, type = "b", col = "#12b59a", pch = 19,
ylim = range(c(train_error_xgb, test_error_xgb)))
lines(1:n_rounds, test_error_xgb, type = "b", col = "#bd552f", pch = 17)
legend("topright", legend = c("Train Error", "Test Error"),
col = c("#12b59a", "#bd552f"), pch = c(19, 17), lty = 1)
test_error_xgb[which.min(test_error_xgb)]
library(rpart)
library(rpart.plot)
# Fit tree
tree_model <- rpart(Acresbuckets ~ .,data = train_df, method = "anova")
# Plot the tree
rpart.plot(tree_model)
